# MLTesting


Enhancing the trust of machine learning applications with large input spaces is a desirable goal; however, due to high labeling costs and limited resources, this is a challenging task. One solution is to use test input prioritization techniques that aim to identify only the most effective test instances and to make them a part of a permanent test case repository. These prioritized test inputs can then be used with some popular testing techniques i.e., Metamorphic testing (MT) to test and uncover implementation bugs in computationally complex machine learning applications that suffer from the oracle problem. However, there are certain limitations involved with this approach, (i) the prioritized test inputs may not be enough to check the program correctness over a diverse set of data, and (ii) traditional MT approaches become infeasible when the programs under test exhibit a non-deterministic behavior during training e.g., Artificial Neural Networks (ANNs). Therefore, instead of using MT for testing purposes, we use it to solve a data generation problem; that is, to double the size of prioritized test inputs without incurring additional labeling costs. Further, we propose a machine learning-based approach to predict faulty behavior in two other machine learning classifiers under test (i.e., Neural Network-based Intrusion Detection Systems) using the prioritized inputs. In our case, the problem is interesting in the sense that the injected bugs represent the high accuracy producing mutated program versions, hence they may be difficult to detect by a software developer. The results obtained show that for both case studies, Random Forest outperforms and achieves the best performance over SVM and k-NN algorithms.
