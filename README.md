# MLTesting


Enhancing the trust of machine learning applications with large input spaces is a desirable goal; however, due to high labeling costs and limited resources, this is a challenging task. One solution is to use test input prioritization techniques that aim to identify only the most effective test instances. These prioritized test inputs can then be used with some popular testing techniques i.e., Metamorphic testing (MT) to test and uncover implementation bugs in computationally complex machine learning applications that suffer from the oracle problem. However, there are certain limitations involved with this approach, (i) the prioritized test inputs may not be enough to check the program correctness over a large variety of input scenarios, and (ii) traditional MT approaches become infeasible when the programs under test exhibit a non-deterministic behavior during training e.g., Artificial Neural Networks (ANNs). Therefore, instead of using MT for testing purposes, we use it to solve a data generation problem; that is, enhancing the test inputs effectiveness by doubling the size of prioritized test inputs without incurring additional labeling costs. Further, we leverage the prioritized test inputs (both source and follow-up data sets) and propose a statistical hypothesis testing (for detection) and machine learning-based approach (for prediction) of faulty behavior in two other machine learning classifiers under test (i.e., Neural Network-based Intrusion Detection Systems). In our case, the problem is interesting in the sense that the injected bugs (using mutation testing technique) represent the high accuracy producing mutated program versions, hence they may be difficult to detect by a software developer. The results obtained show that for both case studies, Random Forest outperforms and achieves the best performance over SVM and k-NN algorithms.
